{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2393ea16-c25b-4e5e-852e-6682b3f6e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from os import chdir\n",
    "import re\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac9a493-4274-47a6-8b13-55ebbd92d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following configuration works well on machines with 256 cores and 1TB memory.\n",
    "# It configures Spark in local mode and uses all the available resources.\n",
    "# To run it on a machine with less memory available, please reduce spark.executor.memory & spark.driver.memory,\n",
    "# and increase spark.default.parallelism & spark.sql.shuffle.partitions to reduce the memory demand.\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.memory\", \"1000g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1000g\") \\\n",
    "    .config(\"spark.local.dir\", \"/mnt/vol1/tmp\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf3bda4-0508-449f-8378-72d900caf016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:4040 256\n"
     ]
    }
   ],
   "source": [
    "# Spark UI url\n",
    "\n",
    "sparkUrlParts = list(urlsplit(sc.uiWebUrl))\n",
    "sparkUrlParts[1] = re.sub('^[^:]*', 'localhost', sparkUrlParts[1])\n",
    "sparkUrl = urlunsplit(sparkUrlParts)\n",
    "\n",
    "print(sparkUrl, sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "519c6ff3-769e-487d-94d4-1dbd7b084a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the corpus csv.\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"../corpus\")\n",
    "\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03a45c7-0fa0-43da-8e40-3526ed014dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation\n",
    "# print(\"Missing title\", df.filter(df.title.isNull()).count(), \"Missing text\", df.filter(df.text.isNull()).count())\n",
    "\n",
    "# Keep minimal data in memory.\n",
    "df = df.select(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad61d540-bbc6-4b60-91e3-82431d773f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"char\", pattern=\".\", gaps=False, minTokenLength=1, toLowercase=False)\n",
    "char_df = regexTokenizer.transform(df).select(\"char\")\n",
    "\n",
    "char_df.cache()\n",
    "char_count = char_df.count()\n",
    "# char_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a54a2d3c-dab1-4dda-9b98-85c6bd3bf99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1443.8739557177414"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_threshold = 10 ** (math.log(char_count, 10) / 2)\n",
    "char_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78ce9ca3-c12d-4b81-be69-b9e6d24b3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate and count all ngrams then write them to a single csv.\n",
    "char_blacklist = \"[\\u0000-\\u0019\\u0021-\\u00FF\\u2000-\\u206F\\u3002\\uff1f\\uff01\\uff0c\\u3001\\uff1b\\uff1a\\u201c\\u201d\\u2018\\u2019\\uff08\\uff09\\u300a\\u300b\\u3008\\u3009\\u3010\\u3011\\u300e\\u300f\\u300c\\u300d\\ufe43\\ufe44\\u3014\\u3015\\u2026\\u2014\\uff5e\\ufe4f\\uffe5\\\"']\"\n",
    "# del ã€€\n",
    "def gen_ngram(n):\n",
    "    ngram_gen = NGram(n=n, inputCol=\"char\", outputCol=\"ngrams_list\")\n",
    "    ngram_df = ngram_gen.transform(char_df).select(explode('ngrams_list').alias('ngrams'))\n",
    "    # ngram_df = ngram_df.groupBy('ngrams').count().orderBy('count', ascending=False)\n",
    "    # prct = ngram_df.agg(percentile_approx('count', [0.5, 0.75, 0.875, 0.9, 0.95, 0.99]))\n",
    "    # print(prct.collect())\n",
    "    # ngram_df = ngram_df.limit(int(ngram_df.count() * 0.25))\n",
    "    ngram_df = ngram_df.filter(~col('ngrams').rlike(char_blacklist)) # Remove ngrams with blacklisted chars.\n",
    "    ngram_df = ngram_df.groupBy('ngrams').count()\n",
    "    ngram_df = ngram_df.filter(ngram_df['count'] >= char_threshold)\n",
    "    #if n > 1:\n",
    "        # By precentile\n",
    "        # ngram_df = ngram_df.limit(int(ngram_df.count() * 0.25))\n",
    "        # ngram_df = ngram_df.filter(ngram_df['count'] >= char_threshold)\n",
    "    \n",
    "    return ngram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4daacd64-8b16-4e1c-8851-717af3d6d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1-ngram...\n",
      "Generating 2-ngram...\n",
      "Generating 3-ngram...\n",
      "Generating 4-ngram...\n",
      "Generating 5-ngram...\n",
      "Generating 6-ngram...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ngrams: string, count: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate ngram with different lengths\n",
    "\n",
    "ngram_result_df = None\n",
    "ngram_max_len = 6\n",
    "\n",
    "for n in range(1, ngram_max_len + 1):\n",
    "    print(\"Generating \" + str(n) + \"-ngram...\")\n",
    "    ngram_n_df = gen_ngram(n)\n",
    "    if ngram_result_df == None:\n",
    "        ngram_result_df = ngram_n_df\n",
    "    else:\n",
    "        ngram_result_df = ngram_result_df.unionByName(ngram_n_df)\n",
    "\n",
    "# ngram_result_df = ngram_result_df.orderBy(col(\"count\").desc())\n",
    "ngram_result_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0d8e9f8-5582-4d03-a4c9-d0929e38791c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128554"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Spark calcuation\n",
    "ngram_result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17752746-acef-4f96-beb7-d320e81dad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the result into Pythonland for processing.\n",
    "ngram_result_rows = ngram_result_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "172b4159-c1b2-409e-8161-dc8a9d4e7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rows into a Counter dict.\n",
    "\n",
    "ngram_result_dict = Counter()\n",
    "\n",
    "for row in ngram_result_rows:\n",
    "    ngrams_str = row.ngrams[::2] # NGram inserts space between chars. Remove them.\n",
    "    has_ascii = any(ord(c) < 256 for c in ngrams_str)\n",
    "    if ' ' in ngrams_str or has_ascii: continue # Filter out any strings containing space\n",
    "    ngram_result_dict[ngrams_str] = row['count']\n",
    "    # print(ngrams_str)\n",
    "    # Calculate total count of single char.\n",
    "    if len(ngrams_str) == 1:\n",
    "        ngram_result_dict[ngrams_str[:-1]] += row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "70968c4a-4897-43cc-9192-c2a309dc04a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14348\n"
     ]
    }
   ],
   "source": [
    "# Calculate freqency from count\n",
    "\n",
    "ngram_prob_dict = dict()\n",
    "ngram_total_prob_dict = dict()\n",
    "ngram_total_prob_dict[''] = 1\n",
    "\n",
    "for ngram_str in ngram_result_dict:\n",
    "    if ngram_str == '': continue\n",
    "    parent_freq = ngram_result_dict[ngram_str[:-1]]\n",
    "    parent_total_prob = ngram_total_prob_dict.get(ngram_str[:-1], 0)\n",
    "    if parent_freq == 0 or parent_total_prob == 0: continue\n",
    "    freq = ngram_result_dict[ngram_str] / parent_freq\n",
    "    total_prob = parent_total_prob * freq\n",
    "    \n",
    "    if freq >= 0.00002 or len(ngram_str) == 1:\n",
    "        ngram_prob_dict[ngram_str] = freq\n",
    "        ngram_total_prob_dict[ngram_str] = total_prob\n",
    "\n",
    "# Remove suffix\n",
    "count = 0\n",
    "for ngram_str in ngram_total_prob_dict:\n",
    "    if len(ngram_str) <= 2: continue\n",
    "    ngram_suffix = ngram_str[1:]\n",
    "    ngram_prob = ngram_total_prob_dict.get(ngram_str, 0)\n",
    "    suffix_prob = ngram_total_prob_dict.get(ngram_suffix, 0)\n",
    "    prob_diff = ngram_prob - suffix_prob\n",
    "    if -1e-6 <= prob_diff and prob_diff <= 1e-6:\n",
    "        del ngram_prob_dict[ngram_suffix]\n",
    "        #print('deleting ' + ngram_suffix + ' from ' + ngram_str)\n",
    "        count += 1\n",
    "        #if count > 290: break\n",
    "\n",
    "# print(count)\n",
    "        \n",
    "ngram_prob_dict[''] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5f00b71-2f06-470a-b4d9-88bda1b9d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the result into a csv file.\n",
    "\n",
    "output_path = \"6gram-prob-nosym-lite.csv\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"ngram,freq,total_freq\\n\")\n",
    "    for ngram_str in ngram_prob_dict:\n",
    "        total_prob = ngram_prob_dict.get(ngram_str[:-1], 0) * ngram_prob_dict.get(ngram_str, 0)\n",
    "        if ngram_str == '': continue\n",
    "        f.write(ngram_str + \",\" + str(ngram_prob_dict[ngram_str]) + \",\" + str(ngram_total_prob_dict[ngram_str]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5454b-8897-40da-894b-45064b371f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
